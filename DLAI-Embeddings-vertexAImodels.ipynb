{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [DLAI - Understanding and Applying Text embeddings](https://learn.deeplearning.ai/courses/google-cloud-vertex-ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project environment setup\n",
    "\n",
    "- Load credentials and relevant Python Libraries\n",
    "- If you were running this notebook locally, you would first install Vertex AI.  In this classroom, this is already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \n",
    "#!pip install google-cloud-aiplatform\n",
    "\n",
    "from utils import authenticate\n",
    "credentials, PROJECT_ID = authenticate() # Get credentials and project ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application of Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextGenerationModel\n",
    "\n",
    "generation_model = TextGenerationModel.from_pretrained(\"text-bison@001\") # for Q&A\n",
    "generation_model = TextGenerationModel.from_pretrained(\"chat-bison@001\") # for chatbots\n",
    "\n",
    "# This is from public list of questions and answers from Stack overflow\n",
    "# question_embeddings has embeddings of the questions only. The code to create this is not present in this file, therefore this section will not work\n",
    "question_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster the embeddings of the Stack Overflow questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "clustering_dataset = question_embeddings[:1000]\n",
    "n_clusters = 2\n",
    "kmeans = KMeans(n_clusters=n_clusters, \n",
    "                random_state=0, \n",
    "                n_init = 'auto').fit(clustering_dataset)\n",
    "kmeans_labels = kmeans.labels_\n",
    "\n",
    "# Reduce dimensionality to help visualize\n",
    "from sklearn.decomposition import PCA\n",
    "PCA_model = PCA(n_components=2)\n",
    "PCA_model.fit(clustering_dataset)\n",
    "new_values = PCA_model.transform(clustering_dataset)\n",
    "\n",
    "# To visualize\n",
    "import matplotlib.pyplot as plt\n",
    "import mplcursors\n",
    "%matplotlib ipympl\n",
    "from utils import clusters_2D\n",
    "clusters_2D(x_values = new_values[:,0], y_values = new_values[:,1], \n",
    "            labels = so_df[:1000], kmeans_labels = kmeans_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Isolation Forest to identify potential outliers\n",
    "\n",
    "- `IsolationForest` classifier will predict `-1` for potential outliers, and `1` for non-outliers.\n",
    "- You can inspect the rows that were predicted to be potential outliers and verify that the question about baking is predicted to be an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "clf = IsolationForest(contamination=0.005, random_state = 2) \n",
    "preds = clf.fit_predict(question_embeddings)\n",
    "\n",
    "print(f\"{len(preds)} predictions. Set of possible values: {set(preds)}\")\n",
    "so_df.loc[preds == -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "- Train a random forest model to classify the category of a Stack Overflow question (as either Python, R, HTML or CSS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# read the category from the file to get Y\n",
    "so_df = pd.read_csv('so_database_app.csv')\n",
    "y = so_df['category'].values\n",
    "y.shape\n",
    "\n",
    "# X would be question_embeddings\n",
    "X = question_embeddings\n",
    "X.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 2)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=200)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred) # compute accuracy\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# choose a number between 0 and 1999\n",
    "i = 2\n",
    "label = so_df.loc[i,'category']\n",
    "question = so_df.loc[i,'input_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs for Question Answering\n",
    "- You can ask an open-ended question to the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I'm a high school student. Recommend me a programming activity to improve my skills.\"\n",
    "generation_model.predict(prompt=prompt).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting Creativity/Randomness\n",
    "- You can control the behavior of the language model's decoding strategy by adjusting the temperature, top-k, and top-n parameters.\n",
    "- For tasks for which you want the model to consistently output the same result for the same input, (such as classification or information extraction), set temperature to zero.\n",
    "- For tasks where you desire more creativity, such as brainstorming, summarization, choose a higher temperature (up to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.0\n",
    "response = generation_model.predict(\n",
    "    prompt=prompt,\n",
    "    temperature=temperature,\n",
    ")\n",
    "generation_model.predict(prompt=prompt).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top P\n",
    "- Top p: sample the minimum set of tokens whose probabilities add up to probability `p` or greater.\n",
    "- The default value for `top_p` is `0.95`.\n",
    "- If you want to adjust `top_p` and `top_k` and see different results, remember to set `temperature` to be greater than zero, otherwise the model will always choose the token with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_p = 0.2\n",
    "prompt = \"Write an advertisement for jackets \\\n",
    "that involves blue elephants and avocados.\"\n",
    "response = generation_model.predict(\n",
    "    prompt=prompt, \n",
    "    temperature=0.9, \n",
    "    top_p=top_p,\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top k\n",
    "- The default value for `top_k` is `40`.\n",
    "- You can set `top_k` to values between `1` and `40`.\n",
    "- The decoding strategy applies `top_k`, then `top_p`, then `temperature` (in that order)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top_k = 20\n",
    "top_p = 0.7\n",
    "response = generation_model.predict(\n",
    "    prompt=prompt, \n",
    "    temperature=0.9, \n",
    "    top_k=top_k,\n",
    "    top_p=top_p,\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs for Semantic Search, Building a Q&A System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a database within python (you can also use vector dbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read external data, here it is in form of a CSV\n",
    "import pandas as pd\n",
    "so_database = pd.read_csv('so_database_app.csv')\n",
    "\n",
    "# Import text embedding model to convert the external text to embeddings\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "embedding_model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\n",
    "\n",
    "# Here is the code that embeds the text\n",
    "import numpy as np\n",
    "from utils import encode_text_to_embedding_batched\n",
    "\n",
    "so_questions = so_database.input_text.tolist()\n",
    "question_embeddings = encode_text_to_embedding_batched(\n",
    "            sentences = so_questions,\n",
    "            api_calls_per_second = 20/60, \n",
    "            batch_size = 5)\n",
    "\n",
    "import pickle\n",
    "with open('question_embeddings_app.pkl', 'rb') as file:\n",
    "      \n",
    "    # Call load method to deserialze\n",
    "    question_embeddings = pickle.load(file)\n",
    "\n",
    "# Add embeddings as a column to the dataframe\n",
    "so_database['embeddings'] = question_embeddings.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Search\n",
    "\n",
    "When a user asks a question, we can embed their query on the fly and search over all of the Stack Overflow question embeddings to find the most simliar datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import pairwise_distances_argmin as distances_argmin\n",
    "\n",
    "query = ['How to concat dataframes pandas']\n",
    "query_embedding = embedding_model.get_embeddings(query)[0].values\n",
    "\n",
    "# Calculate the cosine similarity between query and every embedding (building an index)\n",
    "# First arguemnt is a list of list, there query_embedding in within a parenthesis. second argument is a list, there array is converted to a list \n",
    "cos_sim_array = cosine_similarity([query_embedding], list(so_database.embeddings.values))\n",
    "\n",
    "# This is (1,2000), so index is calculated for the query with every data point\n",
    "cos_sim_array.shape\n",
    "\n",
    "# Grab the question with the max cosine\n",
    "index_doc_cosine = np.argmax(cos_sim_array)\n",
    "index_doc_distances = distances_argmin([query_embedding], list(so_database.embeddings.values))[0]\n",
    "so_database.input_text[index_doc_cosine]\n",
    "so_database.output_text[index_doc_cosine]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question answering with relevant context\n",
    "\n",
    "Now that we have found the most simliar Stack Overflow question, we can take the corresponding answer and use an LLM to produce a more conversational response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextGenerationModel\n",
    "generation_model = TextGenerationModel.from_pretrained(\n",
    "    \"text-bison@001\")\n",
    "\n",
    "context = \"Question: \" + so_database.input_text[index_doc_cosine] +\\\n",
    "\"\\n Answer: \" + so_database.output_text[index_doc_cosine]\n",
    "\n",
    "# if the question isn't actually relevant when answering the user query? \n",
    "# we added additional instructions in the prompt to handle those cases\n",
    "prompt = f\"\"\"Here is the context: {context}\n",
    "             Using the relevant information from the context,\n",
    "             provide an answer to the query: {query}.\"\n",
    "             If the context doesn't provide \\\n",
    "             any relevant information, \\\n",
    "             answer with \\\n",
    "             [I couldn't find a good match in the \\\n",
    "             document database for your query]\n",
    "             \"\"\"\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "t_value = 0.2\n",
    "response = generation_model.predict(prompt = prompt, temperature = t_value, max_output_tokens = 1024)\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale with approximate nearest neighbor search\n",
    "\n",
    "When dealing with a large dataset, computing the similarity between the query and each original embedded document in the database might be too expensive. Instead of doing that, you can use approximate nearest neighbor algorithms that find the most similar documents in a more efficient way.\n",
    "\n",
    "These algorithms usually work by creating an index for your data, and using that index to find the most similar documents for your queries. In this notebook, we will use ScaNN to demonstrate the benefits of efficient vector similarity search. First, you have to create an index for your embedded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scann\n",
    "from utils import create_index\n",
    "\n",
    "#Create index using scann\n",
    "index = create_index(embedded_dataset = question_embeddings, \n",
    "                     num_leaves = 25,\n",
    "                     num_leaves_to_search = 10,\n",
    "                     training_sample_size = 2000)\n",
    "\n",
    "query = \"how to concat dataframes pandas\"\n",
    "\n",
    "import time \n",
    "\n",
    "# Use scann\n",
    "start = time.time()\n",
    "query_embedding = embedding_model.get_embeddings([query])[0].values\n",
    "neighbors, distances = index.search(query_embedding, final_num_neighbors = 1)\n",
    "end = time.time()\n",
    "\n",
    "for id, dist in zip(neighbors, distances):\n",
    "    print(f\"[docid:{id}] [{dist}] -- {so_database.input_text[int(id)][:125]}...\")\n",
    "\n",
    "print(\"Latency (ms):\", 1000 * (end - start))\n",
    "\n",
    "# Use cosine similarity\n",
    "start = time.time()\n",
    "query_embedding = embedding_model.get_embeddings([query])[0].values\n",
    "cos_sim_array = cosine_similarity([query_embedding], list(so_database.embeddings.values))\n",
    "index_doc = np.argmax(cos_sim_array)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"[docid:{index_doc}] [{np.max(cos_sim_array)}] -- {so_database.input_text[int(index_doc)][:125]}...\")\n",
    "\n",
    "print(\"Latency (ms):\", 1000 * (end - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
