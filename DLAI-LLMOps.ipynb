{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Course details: See https://learn.deeplearning.ai/courses/llmops\n",
    "\n",
    "Project environment setup:\n",
    "\n",
    "Load credentials and relevant Python Libraries\n",
    "If you were running this notebook locally, you would first install Vertex AI. In this classroom, this is already installed.\n",
    "!pip install google-cloud-aiplatform\n",
    "You can download the requirements.txt for this course from the workspace of this lab. File --> Open..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import authenticate\n",
    "credentials, PROJECT_ID = authenticate() \n",
    "REGION = \"us-central1\"\n",
    "import vertexai #import the SDK for vertex AI\n",
    "vertexai.init(project = PROJECT_ID,\n",
    "              location = REGION,\n",
    "              credentials = credentials)\n",
    "\n",
    "from google.cloud import bigquery\n",
    "bq_client = bigquery.Client(project=PROJECT_ID, \n",
    "                            credentials = credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Using stack overflow data\n",
    "QUERY = \"\"\"\n",
    "SELECT\n",
    "    CONCAT(q.title, q.body) as input_text,\n",
    "    a.body AS output_text\n",
    "FROM\n",
    "    `bigquery-public-data.stackoverflow.posts_questions` q\n",
    "JOIN\n",
    "    `bigquery-public-data.stackoverflow.posts_answers` a\n",
    "ON\n",
    "    q.accepted_answer_id = a.id\n",
    "WHERE\n",
    "    q.accepted_answer_id IS NOT NULL AND\n",
    "    REGEXP_CONTAINS(q.tags, \"python\") AND\n",
    "    a.creation_date >= \"2020-01-01\"\n",
    "LIMIT\n",
    "    10000\n",
    "\"\"\"\n",
    "\n",
    "query_job = bq_client.query(QUERY)\n",
    "\n",
    "# Take the results of the query --> create an arrow table (which is part of Apache Framework) --> which goes into a Pandas dataframe.\n",
    "# This allows for data to be in a format which is easier to read and explore with Pandas\n",
    "stack_overflow_df = query_job.result()\\\n",
    "                        .to_arrow()\\\n",
    "                        .to_pandas()\n",
    "\n",
    "stack_overflow_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Instructions\n",
    "* Instructions for LLMs have been shown to improve model performance and generalization to unseen tasks (Google, 2022).\n",
    "* Wihtout the instruction, it is only question and answer. Model might not understand what to do.\n",
    "* With the instructions, the model gets a guideline as to what task to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTION_TEMPLATE = f\"\"\"\\\n",
    "Please answer the following Stackoverflow question on Python. \\\n",
    "Answer it like you are a developer answering Stackoverflow questions.\n",
    "\n",
    "Stackoverflow question:\n",
    "\"\"\"\n",
    "# A new column will combine `INSTRUCTION_TEMPLATE` and the question `input_text`.\n",
    "stack_overflow_df['input_text_instruct'] = INSTRUCTION_TEMPLATE + ' '\\\n",
    "    + stack_overflow_df['input_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset for Tuning\n",
    "* Divide the data into a training and evaluation. By default, 80/20 split is used.\n",
    "* The random_state parameter is used to ensure random sampling for a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, evaluation = train_test_split(\n",
    "    stack_overflow_df,\n",
    "    ### test_size=0.2 means 20% for evaluation\n",
    "    ### which then makes train set to be of 80%\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different Datasets and Flow\n",
    "* Versioning data is important.\n",
    "* It allows for reproducibility, traceability, and maintainability of machine learning models.\n",
    "* Get the timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "date = datetime.datetime.now().strftime(\"%H:%d:%m:%Y\")\n",
    "\n",
    "#Using JSON Lines fornat, but TFRecord (which is binary and fast) and Paraquet (large and complex data) can also be used\n",
    "\n",
    "# Training file\n",
    "cols = ['input_text_instruct','output_text']\n",
    "tune_jsonl = train[cols].to_json(orient=\"records\", lines=True)\n",
    "\n",
    "training_data_filename = f\"tune_data_stack_overflow_\\\n",
    "                            python_qa-{date}.jsonl\"\n",
    "\n",
    "#Stored locally, but it is a best practice to store on cloud storage\n",
    "with open(training_data_filename, \"w\") as f:\n",
    "    f.write(tune_jsonl)\n",
    "\n",
    "\n",
    "# Evaluation file\n",
    "cols = ['input_text_instruct','output_text']\n",
    "### you need to use the \"evaluation\" set now\n",
    "tune_jsonl = evaluation[cols].to_json(orient=\"records\", lines=True)\n",
    "\n",
    "### change the file name\n",
    "### use \"tune_eval_data_stack_overflow_python_qa-{date}.jsonl\"\n",
    "evaluation_data_filename = f\"tune_eval_data_stack_overflow_\\\n",
    "                            python_qa-{date}.jsonl\"\n",
    "\n",
    "### write the file\n",
    "with open(evaluation_data_filename, \"w\") as f:\n",
    "    f.write(tune_jsonl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automaton and Orchestation with pipelines\n",
    "Automation and Orchestation with pipelines: We will use [Kubeflow Pipelines](https://www.kubeflow.org/docs/components/pipelines/v2/) to orchestrat and automate a workflow. Kubeflow Pipelines is an open source framework. It's like a construction kit for building machine learning pipelines, making it easy to orchestrate and automate complex tasks.\n",
    "\n",
    "Kubeflow pipelines (can also use Apache Airflow) consist of two key concepts: Components and pipelines. \n",
    "* Ccomponents are like self-contained sets of code that perform various steps in your ML workflow, such as, the first step could be preprocessing data, and second step could be training a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl # domain specific lanugage, this set of instructions and configuration\n",
    "from kfp import compiler # Compiles the instructions\n",
    "\n",
    "# Ignore FutureWarnings in kfp\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \n",
    "                        category=FutureWarning, \n",
    "                        module='kfp.*')\n",
    "\n",
    "TRAINING_DATA_URI = \"./tune_data_stack_overflow_python_qa.jsonl\" \n",
    "EVAUATION_DATA_URI = \"./tune_eval_data_stack_overflow_python_qa.jsonl\"\n",
    "\n",
    "### path to the pipeline file to reuse\n",
    "### the file is provided in your workspace as well\n",
    "template_path = 'https://us-kfp.pkg.dev/ml-pipeline/\\\n",
    "large-language-model-pipelines/tune-large-model/v2.0.0'\n",
    "\n",
    "import datetime\n",
    "date = datetime.datetime.now().strftime(\"%H:%d:%m:%Y\")\n",
    "MODEL_NAME = f\"deep-learning-ai-model-{date}\"\n",
    "\n",
    "# TRAINING_STEPS: Number of training steps to use when tuning the model. For extractive QA you can set it from 100-500. This is no epochs\n",
    "TRAINING_STEPS = 200\n",
    "# EVALUATION_INTERVAL: The interval determines how frequently a trained model is evaluated against the created evaluation\n",
    "EVALUATION_INTERVAL = 20\n",
    "\n",
    "\n",
    "from utils import authenticate\n",
    "credentials, PROJECT_ID = authenticate() \n",
    "REGION = \"us-central1\"\n",
    "pipeline_arguments = {\n",
    "    \"model_display_name\": MODEL_NAME,\n",
    "    \"location\": REGION,\n",
    "    \"large_model_reference\": \"text-bison@001\",\n",
    "    \"project\": PROJECT_ID,\n",
    "    \"train_steps\": TRAINING_STEPS,\n",
    "    \"dataset_uri\": TRAINING_DATA_URI,\n",
    "    \"evaluation_interval\": EVALUATION_INTERVAL,\n",
    "    \"evaluation_data_uri\": EVAUATION_DATA_URI,\n",
    "}\n",
    "\n",
    "pipeline_root \"./\"\n",
    "\n",
    "job = PipelineJob(\n",
    "        ### path of the yaml file to execute\n",
    "        template_path=template_path,\n",
    "        ### name of the pipeline\n",
    "        display_name=f\"deep_learning_ai_pipeline-{date}\",\n",
    "        ### pipeline arguments (inputs)\n",
    "        parameter_values=pipeline_arguments,\n",
    "        ### region of execution\n",
    "        location=REGION,\n",
    "        ### root is where temporary files are being \n",
    "        ### stored by the execution engine\n",
    "        pipeline_root=pipeline_root,\n",
    "        ### enable_caching=True will save the outputs \n",
    "        ### of components for re-use, and will only re-run those\n",
    "        ### components for which the code or data has changed.\n",
    "        enable_caching=True,\n",
    ")\n",
    "\n",
    "### submit for execution\n",
    "job.submit()\n",
    "\n",
    "### check to see the status of the job\n",
    "job.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction, Prompts, Safety\n",
    "We can deploy using a batch or Rest API\n",
    "\n",
    "This part code is not included here\n",
    "See https://learn.deeplearning.ai/courses/llmops"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
