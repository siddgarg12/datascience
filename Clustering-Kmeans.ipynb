{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the article - https://towardsdatascience.com/mastering-customer-segmentation-with-llm-3d9008235f41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install plotly\n",
    "! pip install shap\n",
    "! pip install pyod\n",
    "! pip install yellowbrick\n",
    "! pip install lightgbm\n",
    "! pip install prince"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and importing libraries\n",
    "\n",
    "import pandas as pd # dataframe manipulation\n",
    "import numpy as np # linear algebra\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "# sklearn \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import PowerTransformer, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples, accuracy_score, classification_report\n",
    " \n",
    "from pyod.models.ecod import ECOD # pybd =  Python Outlier Detection (PyOD) library\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "import lightgbm as lgb\n",
    "import prince"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file\n",
    "df = pd.read_csv(\"./data/train.csv\", sep = \";\")\n",
    "df = df.iloc[:, 0:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "categorical_transformer_onehot = Pipeline(\n",
    "    steps=[\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\", sparse_output=False))\n",
    "    ])\n",
    "\n",
    "categorical_transformer_ordinal = Pipeline(\n",
    "    steps=[\n",
    "        (\"encoder\", OrdinalEncoder())\n",
    "    ])\n",
    "\n",
    "num = Pipeline(\n",
    "    steps=[\n",
    "        (\"encoder\", PowerTransformer())\n",
    "    ])\n",
    "\n",
    "preprocessor  = ColumnTransformer(transformers = [\n",
    "                                        ('cat_onehot', categorical_transformer_onehot, [\"default\", \"housing\", \"loan\", \"job\", \"marital\"]),\n",
    "                                        ('cat_ordinal', categorical_transformer_ordinal, [\"education\"]),\n",
    "                                        ('num', num, [\"age\", \"balance\"])\n",
    "                                        ])\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor)]\n",
    "    )\n",
    "pipe_fit = pipeline.fit(df)\n",
    "\n",
    "data = pd.DataFrame(pipe_fit.transform(df), columns = pipe_fit.get_feature_names_out().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "It is crucial that there are as few outliers in our data as Kmeans is very sensitive to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/yzhao062/pyod\n",
    "clf = ECOD() # ECOD method (“empirical cumulative distribution functions for outlier detection”\n",
    "clf.fit(data)\n",
    "outliers = clf.predict(data) \n",
    "\n",
    "data[\"outliers\"] = outliers\n",
    "df[\"outliers\"] = outliers\n",
    "\n",
    "# Data without outliers\n",
    "data_no_outliers = data[data[\"outliers\"] == 0]\n",
    "data_no_outliers = data_no_outliers.drop([\"outliers\"], axis = 1)\n",
    "\n",
    "# Data with Outliers\n",
    "data_with_outliers = data.copy()\n",
    "data_with_outliers = data_with_outliers.drop([\"outliers\"], axis = 1)\n",
    "\n",
    "# Original Data with Outliers\n",
    "df_no_outliers = df[df[\"outliers\"] == 0]\n",
    "df_no_outliers = df_no_outliers.drop([\"outliers\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the disadvantages of using the Kmeans algorithm is that you must choose the number of clusters you want to use. In this case, in order to obtain that data, we will use Elbow Method. It consists of calculating the distortion that exists between the points of a cluster and its centroid. The objective is clear, to obtain the least possible distortion. In this case we use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "# Instantiate the clustering model and visualizer\n",
    "km = KMeans(init=\"k-means++\", random_state=0, n_init=\"auto\")\n",
    "visualizer = KElbowVisualizer(km, k=(2,10))\n",
    " \n",
    "visualizer.fit(data_no_outliers)        # Fit the data to the visualizer\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=5,\n",
    "            init='k-means++', \n",
    "            n_init=10,\n",
    "            max_iter=100, \n",
    "            random_state=42)\n",
    "\n",
    "clusters_predict = km.fit_predict(data_no_outliers)\n",
    "\n",
    "\"\"\"\n",
    "clusters_predict -> array([4, 2, 0, ..., 3, 4, 3])\n",
    "np.unique(clusters_predict) -> array([0, 1, 2, 3, 4])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "\"\"\"\n",
    "The Davies Bouldin index is defined as the average similarity measure \n",
    "of each cluster with its most similar cluster, where similarity \n",
    "is the ratio of within-cluster distances to between-cluster distances.\n",
    "\n",
    "The minimum value of the DB Index is 0, whereas a smaller \n",
    "value (closer to 0) represents a better model that produces better clusters.\n",
    "\"\"\"\n",
    "print(f\"Davies bouldin score: {davies_bouldin_score(data_no_outliers,clusters_predict)}\")\n",
    "\n",
    "\"\"\"\n",
    "Calinski Harabaz Index -> Variance Ratio Criterion.\n",
    "\n",
    "Calinski Harabaz Index is defined as the ratio of the \n",
    "sum of between-cluster dispersion and of within-cluster dispersion.\n",
    "\n",
    "The higher the index the more separable the clusters.\n",
    "\"\"\"\n",
    "print(f\"Calinski Score: {calinski_harabasz_score(data_no_outliers,clusters_predict)}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The silhouette score is a metric used to calculate the goodness of \n",
    "fit of a clustering algorithm, but can also be used as \n",
    "a method for determining an optimal value of k (see here for more).\n",
    "\n",
    "Its value ranges from -1 to 1.\n",
    "A value of 0 indicates clusters are overlapping and either\n",
    "the data or the value of k is incorrect.\n",
    "\n",
    "1 is the ideal value and indicates that clusters are very \n",
    "dense and nicely separated.\n",
    "\"\"\"\n",
    "print(f\"Silhouette Score: {silhouette_score(data_no_outliers,clusters_predict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prince\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "def get_pca_2d(df, predict):\n",
    "\n",
    "    pca_2d_object = prince.PCA(\n",
    "    n_components=2,\n",
    "    n_iter=3,\n",
    "    rescale_with_mean=True,\n",
    "    rescale_with_std=True,\n",
    "    copy=True,\n",
    "    check_input=True,\n",
    "    engine='sklearn',\n",
    "    random_state=42\n",
    "    )\n",
    "\n",
    "    pca_2d_object.fit(df)\n",
    "\n",
    "    df_pca_2d = pca_2d_object.transform(df)\n",
    "    df_pca_2d.columns = [\"comp1\", \"comp2\"]\n",
    "    df_pca_2d[\"cluster\"] = predict\n",
    "\n",
    "    return pca_2d_object, df_pca_2d\n",
    "\n",
    "\n",
    "\n",
    "def get_pca_3d(df, predict):\n",
    "\n",
    "    pca_3d_object = prince.PCA(\n",
    "    n_components=3,\n",
    "    n_iter=3,\n",
    "    rescale_with_mean=True,\n",
    "    rescale_with_std=True,\n",
    "    copy=True,\n",
    "    check_input=True,\n",
    "    engine='sklearn',\n",
    "    random_state=42\n",
    "    )\n",
    "\n",
    "    pca_3d_object.fit(df)\n",
    "\n",
    "    df_pca_3d = pca_3d_object.transform(df)\n",
    "    df_pca_3d.columns = [\"comp1\", \"comp2\", \"comp3\"]\n",
    "    df_pca_3d[\"cluster\"] = predict\n",
    "\n",
    "    return pca_3d_object, df_pca_3d\n",
    "\n",
    "\n",
    "\n",
    "def plot_pca_3d(df, title = \"PCA Space\", opacity=0.8, width_line = 0.1):\n",
    "\n",
    "    df = df.astype({\"cluster\": \"object\"})\n",
    "    df = df.sort_values(\"cluster\")\n",
    "\n",
    "    fig = px.scatter_3d(\n",
    "          df, \n",
    "          x='comp1', \n",
    "          y='comp2', \n",
    "          z='comp3',\n",
    "          color='cluster',\n",
    "          template=\"plotly\",\n",
    "          \n",
    "          # symbol = \"cluster\",\n",
    "          \n",
    "          color_discrete_sequence=px.colors.qualitative.Vivid,\n",
    "          title=title).update_traces(\n",
    "              # mode = 'markers',\n",
    "              marker={\n",
    "                  \"size\": 4,\n",
    "                  \"opacity\": opacity,\n",
    "                  # \"symbol\" : \"diamond\",\n",
    "                  \"line\": {\n",
    "                      \"width\": width_line,\n",
    "                      \"color\": \"black\",\n",
    "                  }\n",
    "              }\n",
    "          ).update_layout(\n",
    "                  width = 800, \n",
    "                  height = 800, \n",
    "                  autosize = True, \n",
    "                  showlegend = True,\n",
    "                  legend=dict(title_font_family=\"Times New Roman\",\n",
    "                              font=dict(size= 20)),\n",
    "                  scene = dict(xaxis=dict(title = 'comp1', titlefont_color = 'black'),\n",
    "                              yaxis=dict(title = 'comp2', titlefont_color = 'black'),\n",
    "                              zaxis=dict(title = 'comp3', titlefont_color = 'black')),\n",
    "                  font = dict(family = \"Gilroy\", color  = 'black', size = 15))\n",
    "        \n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_3d_object, df_pca_3d = get_pca_3d(data_no_outliers, clusters_predict)\n",
    "plot_pca_3d(df_pca_3d, title = \"PCA Space\", opacity=1, width_line = 0.1)\n",
    "print(\"The variability is :\", pca_3d_object.eigenvalues_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we apply the PCA method, since it is a linear algorithm, it is not capable of capturing more complex relationships. \n",
    "Luckily there is a method called t-SNE, which is capable of capturing these complex polynomial relationships. \n",
    "This can help us visualize, since with the previous method we have not had much success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a long time to run\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "sampling_data = data_no_outliers.sample(frac=0.5, replace=True, random_state=1)\n",
    "sampling_clusters = pd.DataFrame(clusters_predict).sample(frac=0.5, replace=True, random_state=1)[0].values\n",
    "\n",
    "df_tsne_3d = TSNE(\n",
    "                  n_components=3, \n",
    "                  learning_rate=500, \n",
    "                  init='random', \n",
    "                  perplexity=200, \n",
    "                  n_iter = 5000).fit_transform(sampling_data)\n",
    "\n",
    "df_tsne_3d = pd.DataFrame(df_tsne_3d, columns=[\"comp1\", \"comp2\",'comp3'])\n",
    "df_tsne_3d[\"cluster\"] = sampling_clusters\n",
    "plot_pca_3d(df_tsne_3d, title = \"PCA Space\", opacity=1, width_line = 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
